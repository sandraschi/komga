# LLM Configuration
#
# This file contains configuration for various LLM providers.
# Enable only the providers you plan to use by setting `enabled: true`.
# API keys and sensitive information should be provided via environment variables.

komga:
  llm:
    # Master switch for LLM functionality
    enabled: ${LLM_ENABLED:false}
    
    # Default provider to use when the application starts
    # Must be one of: OPENAI, OLLAMA, LM_STUDIO, VLLM, GOOGLE_NOTE_LM
    default-provider: ${LLM_DEFAULT_PROVIDER:OPENAI}
    
    # OpenAI Configuration (https://platform.openai.com)
    openai:
      enabled: ${OPENAI_ENABLED:false}
      api-key: ${OPENAI_API_KEY:}  # Required if enabled
      organization-id: ${OPENAI_ORGANIZATION_ID:}  # Optional
      api-url: ${OPENAI_API_URL:https://api.openai.com/v1}
      model: ${OPENAI_MODEL:gpt-4}
      embedding-model: ${OPENAI_EMBEDDING_MODEL:text-embedding-ada-002}
      temperature: ${OPENAI_TEMPERATURE:0.7}
      max-tokens: ${OPENAI_MAX_TOKENS:1000}
      timeout-seconds: ${OPENAI_TIMEOUT:30}
      max-retries: ${OPENAI_MAX_RETRIES:3}
      retry-delay: ${OPENAI_RETRY_DELAY:1000}
      retry-multiplier: ${OPENAI_RETRY_MULTIPLIER:2.0}
      rate-limit-requests-per-minute: ${OPENAI_RATE_LIMIT:60}
    
    # Ollama Configuration (https://ollama.ai/)
    ollama:
      enabled: ${OLLAMA_ENABLED:false}
      api-url: ${OLLAMA_API_URL:http://localhost:11434}
      model: ${OLLAMA_MODEL:llama2}
      temperature: ${OLLAMA_TEMPERATURE:0.7}
      max-tokens: ${OLLAMA_MAX_TOKENS:2000}
      timeout-seconds: ${OLLAMA_TIMEOUT:120}
      context-window: ${OLLAMA_CONTEXT_WINDOW:4096}
    
    # LM Studio Configuration (https://lmstudio.ai/)
    lmstudio:
      enabled: ${LM_STUDIO_ENABLED:false}
      api-url: ${LM_STUDIO_API_URL:http://localhost:1234/v1}
      model: ${LM_STUDIO_MODEL:local-model}
      temperature: ${LM_STUDIO_TEMPERATURE:0.7}
      max-tokens: ${LM_STUDIO_MAX_TOKENS:2000}
      timeout-seconds: ${LM_STUDIO_TIMEOUT:120}
    
    # vLLM Configuration (https://vllm.ai/)
    vllm:
      enabled: ${VLLM_ENABLED:false}
      api-url: ${VLLM_API_URL:http://localhost:8000/v1}
      model: ${VLLM_MODEL:gpt2}
      temperature: ${VLLM_TEMPERATURE:0.7}
      max-tokens: ${VLLM_MAX_TOKENS:2000}
      timeout-seconds: ${VLLM_TIMEOUT:120}
    
    # Google NoteLM Configuration (https://ai.google.dev/)
    google-note-lm:
      enabled: ${GOOGLE_NOTE_LM_ENABLED:false}
      api-key: ${GOOGLE_NOTE_LM_API_KEY:}  # Required if enabled - base64-encoded service account JSON
      project-id: ${GOOGLE_NOTE_LM_PROJECT_ID:}  # Required if enabled
      location: ${GOOGLE_NOTE_LM_LOCATION:us-central1}
      model: ${GOOGLE_NOTE_LM_MODEL:note-lm}
      temperature: ${GOOGLE_NOTE_LM_TEMPERATURE:0.7}
      max-tokens: ${GOOGLE_NOTE_LM_MAX_TOKENS:1024}
      timeout-seconds: ${GOOGLE_NOTE_LM_TIMEOUT:60}
      api-url: ${GOOGLE_NOTE_LM_API_URL:https://us-central1-aiplatform.googleapis.com/v1}
    
    # Common settings applied to all providers
    common:
      # Default system prompt for chat completions
      system-prompt: |
        You are a helpful assistant that analyzes books, comics, and other media.
        Provide detailed, insightful, and accurate responses.
      
      # Maximum number of tokens to generate by default
      default-max-tokens: 1000
      
      # Default temperature (0.0 to 2.0)
      default-temperature: 0.7
      
      # Default timeout in seconds for API requests
      default-timeout: 30
      
      # Whether to enable debug logging
      debug: ${LLM_DEBUG:false}
